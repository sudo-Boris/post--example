<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style id="distill-article-specific-styles">
    <%=require("../static/styles.css") %>
  </style>
  <script src="https://distill.pub/template.v2.js"></script>
</head>

<body>

  <d-front-matter>
    <script type="text/json">
      <%= JSON.stringify(require("./frontmatter.json"), null, 4) %>
    </script>
  </d-front-matter>

  <d-title>
    <h1>Adaptive Reward Penalty in Safe Reinforcement Learning</h1>
    <p>Reward Constrained Proximal Policy Optimization</p>
  </d-title>

  <d-article>

    <d-abstract class="l-page-outset">
      <p>
        In this blog, we dive into the ICLR 2019 paper "Reward Constrained Policy Optimization" by Tessler et al.
        and highlight the importance of adaptive reward shaping in safe reinforcement learning.

        We reproduce the paper's experimental results by implementing Reward Constrained Policy Optimization
        (RCPO)<d-cite key="Tessler2018RCPO"></d-cite> into Proximal Policy
        Optimization (PPO)<d-cite key="Schulman2017PPO"></d-cite>.

        The goal of this blog is to provide researchers and practitioners with (1) a better understanding of
        safe reinforcement learning in terms of
        <a href="http://www.mit.edu/~dimitrib/Constrained-Opt.pdf"><b>constrained optimization</b></a>
        , and (2) how penalized reward fucntions
        can be effectively used to train a robust policy.
      </p>
    </d-abstract>

    <div class="l-screen horizontal-line"></div>

    <d-contents id="toc">
      <div class="toc-wrapper">
        <nav class="figcaption" id="menu">
          <h4>Contents</h4>
          <div><a href="./#Safe-RL">Introduction to Safe Reinforcement Learning</a></div>
          <div><a href="./#What-are-CMDPs">What are Constrained MDPs</a></div>
          <div><a href="./#CPO">Constrained Policy Optimization</a></div>
          <div><a href="./#RCPO">Reward Constrained Policy Optimization</a></div>
          <div><a href="./#Implementation">Implementation</a></div>
          <div><a href="./#Experiments">Experiments</a></div>
          <div><a href="./#Discussion">Discussion</a></div>
        </nav>
        <div class="toc-line"></div>
      </div>
    </d-contents>

    <h2 id="Safe-RL">
      Introduction to Safe Reinforcement Learning
    </h2>

    <p>
      Safe RL can be defined as the process of learning policies that maximize the expectation
      of the return in problems in which it is important to ensure reasonable system performance
      and/or respect safety constraints during the learning and/or the deployment processes <d-cite
        key="garcia_comprehensive_2015"></d-cite>.
    </p>

    <!-- include video in videos/RL_boat_racing-->
    <div style="display: flex; flex-direction: column; align-items: center;">
      <figure id="RL_boat_racing" class="l-page-outset">
        <img src="videos/RL_boat_racing.mp4" alt="Clip of Open AIs CoastRunners agent." />
      </figure>
      <figcaption>
        Open AIs CoastRunners agent from their blog post <a href=https://openai.com/blog/faulty-reward-functions>"Faulty
          Reward Functions in the Wild"</a> in Dec 2016.
      </figcaption>
    </div>

    <p>
      Defining a reward function is crucial in <a href="https://lilianweng.github.io/posts/2018-02-19-rl-overview/"> Reinforcement Learning</a>
      for solving many problems of interest in AI. It is often based on the designers' intuition of
      the goal of the system. In the above expample of CoastRunners, the goal is to reach the finish line and collect points along the
      way. Whilst this is a reasonable reward function for the goal of winning the game, it allows for dangerous and
      harmful behavior as visible in the video above. The agent is able to drive off the track, crash into other boats,
      and catch fire and still win the game whilst achieving a score on average 20 percent higher than that achieved by
      human players. <br>

    </p>


    <!-- 
    <d-figure id="minigame" class="l-page-outset">
      <div id="minigame-target"></div>
    </d-figure> -->

    <p>
      <b>How can we prevent the agents from violating safety constrains (e.g., crash into other boats)?<br></b>
      Recent studies start to address the problem of safe reinforcement learning from various perspectives,
      ICLR works including, but not limited to:
      <br>
      <a href="https://openreview.net/pdf?id=HJgEMpVFwB">
        Adversarial Policies: Attacking Deep Reinforcement Learning</a>,
      Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, and Stuart Russell, <i>ICLR 2020</i>
      <br>
      <a href="https://arxiv.org/pdf/2201.09802.pdf">
        Constrained Policy Optimization via Bayesian World Models</a>,
      Yarden As, Ilnura Usmanova, Sebastian Curi and Andreas Krause, <i>ICLR 2022</i>
      <br>
      <a href="https://openreview.net/pdf?id=TBIzh9b5eaz">
        Risk-averse Offline Reinforcement Learning</a>,
      Núria Armengol Urpí, Sebastian Curi, and Andreas Krause, <i>ICLR 2021</i>
      <br>
      <a href="https://openreview.net/pdf?id=S1vuO-bCW">
        Leave No Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning</a>,
      Benjamin Eysenbach, Shixiang Gu, Julian Ibarz, and Sergey Levine, <i>ICLR 2018</i>
      <br>
      <a href="https://openreview.net/pdf?id=iaO86DUuKi">
        Conservative Safety Critics for Exploration</a>,
      Homanga Bharadhwaj, Aviral Kumar, Nicholas Rhinehart, Sergey Levine, Florian Shkurti, and Animesh Garg, <i>ICLR
        2021</i>
      <br>
      <a href="https://openreview.net/pdf?id=TQt98Ya7UMP">
        Balancing Constraints and Rewards with Meta-gradient D4PG</a>,
      Dan A. Calian, Daniel J. Mankowitz, Tom Zahavy, Zhongwen Xu, Junhyuk Oh, Nir Levine, and Timothy Mann, <i>ICLR
        2021</i>
      <br>
    </p>

    <p>
      In this blog post we will discuss a simple but effective technique of introducing a guiding penalty into the reward
      function. This method effectively trades off between meeting the constraint and achieving the goal. <br>
    </p>
    </h2>

    <h2 id="What-are-CMDPs">
      A Formalism for Safe Reinforcement Learning: Constrained MDPs
    </h2>

    <d-figure id="MDP">
      <figure>
        <img src="images/MDP.png" alt="Illustration of a Markov Decision Process">
        <figcaption>
          Illustration of a Constrained Markov Decision Process (MDP). <br>
          Based on an observation (also called state) from the environment, the agent selects an action. This action is
          performed in an environment results in a new state and a reward that evaluates the action. Given the new
          state the feedback loop repeats.
        </figcaption>
      </figure>
    </d-figure>

    <p>
      In Reinforcement Learning, the world is modeled as a Markov Decision Process (MDP) and the goal is to
      select a policy <d-math> \pi </d-math> which maximize an expected cumulative reward <d-math>J^π</d-math>.

  <d-math>J^π</d-math> can be taken to be the infinite horizen discounted total return as
      </p>
    <d-math>
      J^π = \mathbb{E}_{s\sim\mu} \left[ \sum_{t=0}^T \gamma^t r(s_t, a_t) \right]
    </d-math>

    <p>
      where  <d-math>\gamma</d-math> is the discount factor, and
      <d-math>r(s_t,a_t)</d-math> is the reward function.
    </p>

    <p>
      However, in many real-world applications, the agents must obey safety constrains while achieving the goal.
      We can introduce a constraint objective analogous to the reward objective.
      This objective is typically defined as the expected constraint value over N time steps
      <d-math>J^π_C = \mathbb{E}_{s\sim\mu} \left[ C(s) \right]</d-math>.
      The method of aggregating individual constraints over time can vary, e.g. using the average
      or the maximum constraint value over N time steps or again a discounted sum.
    </p>

    <p>
      In the example of the robot, the aim could be to prolong the motor life of the various robots,
      while still enabling the robot to perform the task at hand. To do so, the robot motors need to be constrained
      from using high torque values. This is accomplished by defining the constraint C as the average torque
      the agent has applied to each motor, and the pertate penalty <d-math>c(s, a)</d-math>
      becomes the amount of torque the agent decided to apply at each time step.

      If we limit the allowable amount of torque applied to be <d-math>\alpha</d-math>.
    </p>

    <p>
      The cosntrained CMDP for our safe reinforcement learning problem is:
    </p>

    <d-math>
      \max_{\pi \in \Pi} J^π_R \text{ s.t. }
      J^π_C \leq \alpha
    </d-math>

    <h2 id="CPO">
      Constrained Policy Optimization
    </h2>
    <p>
      Constrained RLs are often solved using the Lagrange relaxation technique. With parameterized approaches such as Neural Networks the objective is then to
      find the networks parameters <d-math>\theta</d-math> that maximize <d-math>J^π_R</d-math> subject to the
      constraint
      <d-math>J^π_C \leq \alpha</d-math> given the Lagrangian multiplier <d-math>\lambda</d-math>:
    </p>
    <d-math>
      \min_{\lambda}\max_{\theta} [J^{π_\theta}_R - \lambda (J^{π_\theta}_C - \alpha)]
    </d-math>


    <h3>
      What What exactly does the <d-math>\lambda</d-math> <i><b> </b></i> do?
    </h3>

    <p>
      Intuitively, the Lagrangian multiplier <d-math>\lambda</d-math> is used to determine how much weight is put
      onto the constraint. If <d-math>\lambda</d-math> is set to 0, the constraint is ignored and the objective
      becomes the reward objective <d-math>J^π_R</d-math>. If <d-math>\lambda</d-math> is set very high, the
      constraint is enforced very strictly and the global objective function reduces to the constraint objective
      <d-math>J^π_C</d-math>.
    </p>

    <p>
      Let's take a look at a simple example to <b>demonstrate the effect of the Lagrangian multiplier <d-math>\lambda</d-math> </b>.
      We'll use the simple CartPole Gym environment. The reward in this environment is +1 for every step the pole was
      kept upright.

      We can now add an example constraint to the environment. Let's say we want to keep the cart in the left quarter
      of the x-axis. We therefor define the constraint value to be the x-position of the cart and the upper bound
      <d-math>\alpha</d-math> to be -2.

    <p>
      Try out the different Lambda values in the slider below to see how the constraint is enforced.
    </p>

    <d-figure id="tuning-lambda">
      <figure>
        <div id="tuning-lambda-target" class="video/mp4"></div>
        <figcaption>
          The green area represents the "safe zone", where the x-position is smaller than -2 and the red area the
          "unsafe zone". <br>
          The lower the lambda, the more the constraint is ignored.
          The higher the lambda, the more the constraint is enforced and the main reward objective is ignored.
          At <d-math>\lambda = 1,000,000</d-math> the cart shoots to the right to tilt the pole to the left but does
          so ignoring the following balancing act which is observable at <d-math>\lambda \in \{10, 100\}</d-math>.
        </figcaption>
      </figure>
    </d-figure>

    <p>
      Tuning the <d-math>\lambda</d-math> through <a href = "https://gibberblot.github.io/rl-notes/single-agent/reward-shaping.html">reward shaping </a> is no easy feat. <br>

    <h3>
      How can we learn an optimal <d-math>\lambda</d-math>?
    </h3>

    </p>

    <h2 id="RCPO">
      Reward Constrained Policy Optimization
    </h2>
    <p>
      Actor-Critic based approaches such as <a href ="https://spinningup.openai.com/en/latest/algorithms/ppo.html"
      >PPO</a> has been shown empirically competitive with lots of quality benchmarks. In this kind of algorithms,
      the actor learns a policy <d-math>lr_{\pi}</d-math>, whereas the critic learns the value using temporal difference learning.
      Originally, the use of the critic was done to reduce the variance but it also enables
      training using a finite number of samples.
    </p>

    <h3>
      How to integrate the constraint into the Actor-Critic approach?
    </h3>

    <p>
      One way to integrate the constraint into the Actor-Critic approach is using an alternative, guiding penalty
      - the discounted penalty.
    </p>

    <p>
      The first step to integrate the constraint into the Actor-Critic approach is to use the
      constraint objective <d-math>J^π_C</d-math> as the value function. This is simply done by reformulating it as
      the
      expected discounted sum of the constraint values:
    </p>

    <d-math>
      V^π_{C_\gamma}(s) \hat{=} \mathbb{E}^{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t c(s_t, a_t) | s_0 = s \right]
    </d-math>

    <p>
      Now we can simply include the constraint value to the reward function via the Lagrange multiplier:
    </p>

    <d-math>
      \hat{r} = r(s,a) - \lambda c(s,a)
    </d-math>

    <p>
      This is the guiding penalty. The constraint is now integrated into the reward function and the actor can learn
      to maximize the reward while considering the constraint.
    </p>

    <p>
      Updating the Lagrangian multiplier is done via the derivative of the global objective:
    </p>

    <d-math>
      <d-math>\lambda</d-math> <d-math>\gets</d-math> max(<d-math>\lambda</d-math> -
      lr_{<d-math>\lambda</d-math>}(-(\mathbb{E}^{\pi_\theta}_{s\sim\mu}
      \left[C\right] - \alpha)), 0)
    </d-math>
    <p>
      <d-math>lr_\lambda</d-math> is the learning rate for the Lagrangian multiplier. The max function is
      used to ensure
      that the Lagrangian multiplier is always positive.
    </p>

    <h2 id="Implementation">
      Implementation
    </h2>

    <p>
      To facilitate reproducibility, we integrated RCPO into the stable-baselines3<d-cite key="stable-baselines3"></d-cite> PPO implementation.
    </p>

    <h4>
      Integrating the guiding penalty
    </h4>

    <p>
      In PPO the computation of returns, we use Temporal Difference Error (TD estimate) and
      the Generalized Advantage Estimation (GAE) advantage. <br>
      To integrate the constraint into the reward function, we need to add the Lagrangian-scaled constraint value to
      the reward as discussed in the <a href="./#RCPO">RCPO section</a>. This is done when computing the TD error
      estimate.
    </p>

    <d-figure id="code_snapshot">
      <figure>
        <img src="images/code-snapshot2_new.png" alt="Code snapshot of constraint integration into reward">
        <figcaption>
          The discussed integration of the constraint into the reward function is implemented into the computation of
          the advantages and returns. When the lambda parameter is set to 0, the constraint is ignored and the reward
          function is the same as in the original PPO implementation.
        </figcaption>
      </figure>
    </d-figure>


    <p>
      We customized the gym environments to return the constraint values in the <d-math>info</d-math>
      dictionary.
    </p>

    <h4>
      Updating the Lagrangian multiplier
    </h4>

    <p>
      Due to the fact, that PPO (i) collects multiple episodes until the buffers are full and (ii) supports vectorized
      environments the logic for collecting and aggregating the constraint values across the episodes and parallel
      environments is a bit more complex. Nevertheless, we have chosen the aggregation method to be the average over
      all
      time steps in one complete episode and across all those episodes themselves, i.e. episodes that have reached a
      terminal state.
    </p>

    <d-figure id="code_snapshot_lambda_update">
      <figure>
        <img src="images/lambda_update2_new.png" alt="Code snapshot of Lagrangian update">
        <figcaption>
          After aggregating the constraint values across the episodes and parallel environments into self.C, the
          Lagrangian is updated. The max function is used to ensure that the Lagrangian multiplier is always positive.
        </figcaption>
      </figure>
    </d-figure>

    <h2 id="Experiments">
      Experiments
    </h2>

    <p>
      As a proof-of-the-principle experiment, we reproduced the HalfCheetah task in
      <a href="https://gymnasium.farama.org/environments/mujoco/">OpenAI MuJoCo Gym</a>
      from Tessler C. et al.<d-cite key="Tessler2018RCPO"></d-cite>.
    </p>

    <p>
      The results of the experiments are shown in the following figures. We kept (almost) all hyperparameters the
      same as well as in the original paper and let the agents train for <d-math>1,000,000</d-math> time-steps.
    </p>

    <d-figure id="experiments_results">
      <figure>
        <!-- <img src="images/experiments_results.png" alt="Experiment Results"> -->
        <img src="images/experiments_results_smooth_constraints.png" alt="Experiment Results">
        <figcaption>
          Rewards and average torque of the experiments on the HalfCheetah environment. The maximum torque constraint
          is represented by the dashed line.
        </figcaption>
      </figure>
    </d-figure>

    <p>
      The results demonstrate that the RCPPO trained an agent that successfully walked forward while
      respecting the safety constraint. We achieved comparable results to the original experiments in the paper. <br>
      Interestingly, low <d-math>\lambda</d-math> values seem to be less stable than high <d-math>\lambda</d-math>
      values. The guiding penalty appears to not only be enforcing the constraint but also improves the learning
      process overall.
      This might be due to the fact that the neural network architecture used in the paper is relatively small
      (i.e., 2 layers of 64 hidden units). <br>
    </p>

    <h4>
      Deviating hyperparameters
    </h4>

    <p>
      Furthermore, we had to select higher values for the Lagrangian multiplier itself when performing
      reward shaping, thus also having to increase its respective learning rate when training it as a parameter, so
      that it can grow quicker. This leads to <d-math>lr_{\lambda}</d-math> being larger than
      <d-math>lr_{\pi}</d-math> which ignores one of the assumption made in the paper, yet leads to coherent, and
      better results!
      <br>
      E.g. in the paper a <d-math>\lambda</d-math> value of 0.1 is already very high as it leads to a reward of
      <d-math>-0.4</d-math> and torque of <d-math>0.1387</d-math>, whereas in our case a <d-math>\lambda</d-math>
      value of 1.0 leads to a reward of about <d-math>1 500</d-math> with an average torque of <d-math>0.39</d-math>.
      <br>
      <br>
      A reason for the slower and weaker impact of the constraint could be attributed to the clipping of the trust
      region. This is a technique to ensure that the policy does not change too much between updates and run the risk
      of
      landing in a bad local minimum it can not escape from. This is done by clipping the policy update to a certain
      range. Therefore, even with "high" values of lambda w.r.t. the original paper the policy will not change
      significantly to conform to the constraint.
    </p>

    <h4>
      Qualitative observations
    </h4>

    <p>
      Finally, we want to see if we can qualitatively observe the effect of the constraint on the policy. To do so, we
      have recorded videos of the agents walking forward with different <d-math>\lambda</d-math> values. The results
      can be seen below.
    </p>

    <d-figure id="experiments">
      <figure>
        <div id="experiments-target" class="video/mp4"></div>
        <figcaption>
          The lower the lambda, the more the constraint is ignored.
          The higher the lambda, the more the constraint is enforced and the main reward objective is ignored.
          At <d-math>\lambda \in \{10, 100\}</d-math> the robot applies <d-math>0</d-math> torque to completely oblige
          to the constraint ignoring the main reward objective to walk forward which is observable at <d-math>\lambda
            \in\{RCPPO, 0, 0.00001\}</d-math>. With <d-math>\lambda \in \{0, 0.00001\}</d-math> the robot is able to
          walk forward, but it is visible that it moves its legs much quicker and more aggressively than the RCPPO
          agent. Furthermore, the RCPPO agent walks perfectly, whilst the other (moving) agents tumble over their own
          hecktick steps.
        </figcaption>
      </figure>
    </d-figure>

    <!-- For whatever reason this needs to be included otherwise the viz above won't appear -->
    <d-figure id="svelte-example-dfigure">
    </d-figure>

    <h2 id="Discussion">
      Discussion
    </h2>

    <p>
      The results of the experiments show that the RCPO approach is able to learn a policy that is able to optimize
      the main reward objective while respecting the constraint.
    </p>

    <p>
      Safe Reinforcement Learning is a critical area of research in the field of artificial intelligence, as it has
      the potential to shape the future of autonomous systems in a multitude of domains, ranging from robotics to
      finance. <br>
      The complexer systems become the more difficult it is to ensure safety requirements, especially through
      simple reward shaping. An approach such as RCPO can help to ensure that the safety constraints are respected
      while enforcing them by only providing the constraint itself. <br>
    </p>

    <!-- <h3>
      Parameter updating frequency
    </h3>

    <p>
      To be able to reproduce the results in the paper we had to deviate from it in some aspects. <br><br>
      In our implementation, we only update the weights and the Lagrangian multiplier once after several episode
      instead of after every episode as in the paper. This is due to the implementation design of the buffers in
      stable-baselines3. As mentioned, those need to be filled before the update can be performed, i.e. there will
      most likely be multiple episodes in on buffer. On the other hand, we update the lambda parameter for n epochs,
      just like the policy and value function.
    </p>

    <h3>
      Deviating hyperparameters
    </h3>

    <p>
      Furthermore, we had to select higher values for the Lagrangian multiplier itself when performing
      reward shaping, thus also having to increase its respective learning rate when training it as a parameter, so
      that it can grow quicker. This leads to <d-math>lr_{\lambda}</d-math> being higher than
      <d-math>lr_{\pi}</d-math> which ignores one of the assumption made in the paper, yet leads to coherent, and
      better results!
      <br>
      E.g. in the paper a <d-math>\lambda</d-math> value of 0.1 is already very high as it leads to a reward of
      <d-math>-0.4</d-math> and torque of <d-math>0.1387</d-math>, whereas in our case a <d-math>\lambda</d-math>
      value
      of 1.0 leads to a reward of about <d-math>2,400</d-math> with an average torque of <d-math>0.34</d-math>.
      <br>
      <br>
      In addition to the frequency of updating the parameters, a reason for the slower and weaker impact of the
      constraint could be attributed to the clipping of the trust region. This is a technique to ensure that the
      policy
      does not change too much between updates and run the risk of landing in a bad local minimum it can not escape
      from. This is done by clipping the policy update to a certain range. Therefore, even with "high" values of
      lambda
      w.r.t. the original paper the policy will not change significantly to conform to the constraint.

    </p>

    <h3>
      The Drawbacks of Reward Shaping
    </h3>

    <p>
      Similar to the findings in the paper, we have also observed that the non-adaptive approach is prone to
      converge
      to
      sub-optimal solutions. This is observable when comparing the results of different attempts of training the
      policy
      with the same fixed <d-math> \lambda</d-math> value, e.g. 2. The results are shown in the following figure.
    </p> -->

    <d-appendix>
      <h3>Acknowledgments</h3>

      <p>
        We thank ....
      </p>


      <d-footnote-list></d-footnote-list>
      <d-citation-list></d-citation-list>
    </d-appendix>

    <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
    <d-bibliography src="bibliography.bib"></d-bibliography>

</body>