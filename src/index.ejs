<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style id="distill-article-specific-styles">
    <%=require("../static/styles.css") %>
  </style>
  <script src="https://distill.pub/template.v2.js"></script>
</head>

<body>

  <d-front-matter>
    <script type="text/json">
      <%= JSON.stringify(require("./frontmatter.json"), null, 4) %>
    </script>
  </d-front-matter>

  <d-title>
    <h1>Adaptive Reward Shaping in Safe Reinforcement Learning</h1>
    <p>Reward Constrained Proximal Policy Optimization</p>
  </d-title>

  <d-article>

    <d-abstract class="l-page-outset">
      <p>
        In this blog, we dive into the ICLR 2019 paper "Reward Constrained Policy Optimization" by Tessler et al.
        and highlight the importance of adaptive reward shaping in safe reinforcement learning.

        We reproduce the paper's experimental results by implementing Reward Constrained Policy Optimization
        (RCPO)<d-cite key="Tessler2018RCPO"></d-cite> into Proximal Policy
        Optimization (PPO)<d-cite key="Schulman2017PPO"></d-cite>.

        The goal of this blog is to provide researchers and practitioners with (1) a better understanding of
        safe reinforcement learning in terms of
        <a href="http://www.mit.edu/~dimitrib/Constrained-Opt.pdf"><b>constrained optimization</b></a>
        , and (2) how reward shaping
        can be effectively used to train a robust policy.
      </p>
    </d-abstract>

    <div class="l-screen horizontal-line"></div>

    <d-contents id="toc">
      <div class="toc-wrapper">
        <nav class="figcaption" id="menu">
          <h4>Contents</h4>
          <div><a href="./#Safe-RL">Introduction to Safe Reinforcement Learning</a></div>
          <div><a href="./#What-are-CMDPs">What are Constrained MDPs</a></div>
          <div><a href="./#CPO">Constrained Policy Optimization</a></div>
          <div><a href="./#RCPO">Reward Constrained Policy Optimization</a></div>
          <div><a href="./#Implementation">Implementation</a></div>
          <div><a href="./#Experiments">Experiments</a></div>
          <div><a href="./#Discussion">Discussion</a></div>
        </nav>
        <div class="toc-line"></div>
      </div>
    </d-contents>

    <h2 id="Safe-RL">
      Introduction to Safe Reinforcement Learning
    </h2>

    <p>
      Safe RL can be defined as the process of learning policies that maximize the expectation
      of the return in problems in which it is important to ensure reasonable system performance
      and/or respect safety constraints during the learning and/or the deployment processes <d-cite
        key="garcia_comprehensive_2015"></d-cite>.
    </p>

    <!-- include video in videos/RL_boat_racing-->
    <div style="display: flex; flex-direction: column; align-items: center;">
      <figure id="RL_boat_racing" class="l-page-outset">
        <img src="videos/RL_boat_racing.mp4" alt="Clip of Open AIs CoastRunners agent." />
      </figure>
      <figcaption>
        Open AIs CoastRunners agent from their blog post <a href=https://openai.com/blog/faulty-reward-functions>"Faulty
          Reward Functions in the Wild"</a> in Dec 2016.
      </figcaption>
    </div>

    <p>
      Defining a reward function is a crucial step in the RL process and it is often based on the designers intuition of
      the goal of the system. In the case of CoastRunners, it is to reach the finish line and collect points along the
      way. Whilst this is a reasonable reward function for the goal of winning the game, it allows for dangerous and
      harmful behavior as visible in the video above. The agent is able to drive off the track, crash into other boats,
      and catch fire and still win the game whilst achieving a score on average 20 percent higher than that achieved by
      human players. <br>
      Safe RL aims to address this problem by ensuring that the agent does not violate safety constraints during the
      training process. In this blog post we will discuss the technique of introducing a guiding penalty into the reward
      function and learning the perfect balance between meeting the constraint whilst maximizing the goal. <br>
      In the case of CoastRunners this constraint might be to stay on the track and not crash into other boats.
    </p>


    <!-- 
    <d-figure id="minigame" class="l-page-outset">
      <div id="minigame-target"></div>
    </d-figure> -->

    <p>
      <b>Some other recent ICLR papers on Safe Reinforcement Learning:</b>
      <br>
      <a href="https://openreview.net/pdf?id=HJgEMpVFwB">
        Adversarial Policies: Attacking Deep Reinforcement Learning</a>,
      Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, and Stuart Russell, <i>ICLR 2020</i>
      <br>
      <a href="https://arxiv.org/pdf/2201.09802.pdf">
        Constrained Policy Optimization via Bayesian World Models</a>,
      Yarden As, Ilnura Usmanova, Sebastian Curi and Andreas Krause, <i>ICLR 2022</i>
      <br>
      <a href="https://openreview.net/pdf?id=TBIzh9b5eaz">
        Risk-averse Offline Reinforcement Learning</a>,
      Núria Armengol Urpí, Sebastian Curi, and Andreas Krause, <i>ICLR 2021</i>
      <br>
      <a href="https://openreview.net/pdf?id=S1vuO-bCW">
        Leave No Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning</a>,
      Benjamin Eysenbach, Shixiang Gu, Julian Ibarz, and Sergey Levine, <i>ICLR 2018</i>
      <br>
      <a href="https://openreview.net/pdf?id=iaO86DUuKi">
        Conservative Safety Critics for Exploration</a>,
      Homanga Bharadhwaj, Aviral Kumar, Nicholas Rhinehart, Sergey Levine, Florian Shkurti, and Animesh Garg, <i>ICLR
        2021</i>
      <br>
      <a href="https://openreview.net/pdf?id=TQt98Ya7UMP">
        Balancing Constraints and Rewards with Meta-gradient D4PG</a>,
      Dan A. Calian, Daniel J. Mankowitz, Tom Zahavy, Zhongwen Xu, Junhyuk Oh, Nir Levine, and Timothy Mann, <i>ICLR
        2021</i>
      <br>
    </p>
    </h2>

    <h2 id="What-are-CMDPs">
      What are Constrained MDPs?
    </h2>

    <d-figure id="MDP">
      <figure>
        <img src="images/MDP.png" alt="Illustration of a Markov Decision Process">
        <figcaption>
          Illustration of a Markov Decision Process (MDP). <br>
          Based on an observation (also called state) from the environment, the agent selects an action. This action is
          performed in an environment results in a new state and a reward that evaluates the action. Given the new
          state the feedback loop repeats.
        </figcaption>
      </figure>
    </d-figure>

    <p>
      In Reinforcement Learning (RL) the world is modeled as a Markov Decision Process (MDP) and the goal is to
      optimize an objective <d-math>J^π</d-math>,
      which is defined as the expected discounted reward to come:
    </p>

    <d-math>
      J^π = \mathbb{E}_{s\sim\mu} \left[ \sum_{t=0}^T \gamma^t r(s_t, a_t) \right]
    </d-math>

    <p>
      where <d-math>\pi</d-math> is the policy, <d-math>\gamma</d-math> is the discount factor, and
      <d-math>r(s_t,a_t)</d-math> is the reward function.
    </p>

    <p>
      However, in many real-world applications, the objective is not only to maximize the reward, but also to satisfy
      some constraints. For example, in robotics, the objective is to maximize the reward, e.g. the distance traveled,
      while also satisfying the constraint that the robot should only apply a certain maximum amount
      of torque to its joints.
    </p>

    <p>
      Mathematically speaking, we introduce a new constraint objective and add new indices to differentiate
      between the reward objective <d-math>J^π_R</d-math> and the new constrain objective <d-math>J^π_C</d-math>.
      This objective generally is defined as the expected constraint value over N time steps
      <d-math>J^π_C = \mathbb{E}_{s\sim\mu} \left[ C(s) \right]</d-math>.
      The method of aggregating those individual constraint values over time can vary, e.g. using the average
      or the maximum constraint value over N time steps or again a discounted sum.
    </p>

    <p>
      Getting back to the example of the robot, the constraint objective could be defined as the average torque
      applied
      to the joints over N time steps. The maximum amount of torque applied, i.e. the upper bound of the constraint, is
      given by the parameter <d-math>\alpha</d-math>.
    </p>

    <p>
      The final objective is now to maximize <d-math>J^π_R</d-math> subject to the constraint <d-math>J^π_C \leq
        \alpha</d-math>:
    </p>

    <d-math>
      \max_{\pi \in \Pi} J^π_R \text{ s.t. }
      J^π_C \leq \alpha
    </d-math>

    <h2 id="CPO">
      Constrained Policy Optimization
    </h2>
    <p>
      The problem of optimizing a constrained objective is known as Constrained Policy Optimization (CPO).
      The most common approach to CPO is to use a Lagrangian multiplier <d-math>\lambda</d-math> to
      enforce the constraint. With parameterized approaches such as Neural Networks the objective is then to
      find the networks parameters <d-math>\theta</d-math> that maximize <d-math>J^π_R</d-math> subject to the
      constraint
      <d-math>J^π_C \leq \alpha</d-math> given the Lagrangian multiplier <d-math>\lambda</d-math>:
    </p>
    <d-math>
      \max_{\theta} [J^{π_\theta}_R - \lambda (J^{π_\theta}_C - \alpha)]
    </d-math>

    <h3>
      What does the lambda <i><b>actually</b></i> do?
    </h3>
    <p>
      Intuitively, the Lagrangian multiplier <d-math>\lambda</d-math> is used to determine how much weight is put
      onto the constraint. If <d-math>\lambda</d-math> is set to 0, the constraint is ignored and the objective
      becomes the reward objective <d-math>J^π_R</d-math>. If <d-math>\lambda</d-math> is set very high, the
      constraint is enforced very strictly and the global objective function reduces to the constraint objective
      <d-math>J^π_C</d-math>.
    </p>

    <h4>
      Demonstrating the effect of the Lagrangian multiplier
    </h4>
    <p>
      Let's take a look at a simple example to demonstrate the effect of the Lagrangian multiplier.
      We'll use the simple CartPole Gym environment. The reward in this environment is +1 for every step the pole was
      kept upright.

      We can now add an example constraint to the environment. Let's say we want to keep the cart in the left quarter
      of the x-axis. We therefor define the constraint value to be the x-position of the cart and the upper bound
      <d-math>\alpha</d-math> to be -2.

    <p>
      Try out the different Lambda values in the slider below to see how the constraint is enforced.
    </p>

    <d-figure id="tuning-lambda">
      <figure>
        <div id="tuning-lambda-target" class="video/mp4"></div>
        <figcaption>
          The green area represents the "safe zone", where the x-position is smaller than -2 and the red area the
          "unsafe zone". <br>
          The lower the lambda, the more the constraint is ignored.
          The higher the lambda, the more the constraint is enforced and the main reward objective is ignored.
          At <d-math>\lambda = 1,000,000</d-math> the cart shoots to the right to tilt the pole to the left but does
          so ignoring the following balancing act which is observable at <d-math>\lambda \in \{10, 100\}</d-math>.
        </figcaption>
      </figure>
    </d-figure>

    <h4>
      Reward Shaping: Setting the Lagrangian as a hyperparameter
    </h4>
    <p>
      One can tune the Lagrangian as a hyperparameter (which is then called reward shaping), but ideally one would
      want
      it to be learned itself to find the
      perfect balance between optimizing <d-math>J^π_R</d-math> and <d-math>J^π_C</d-math>!
      So the new and final objective is to find the networks parameters <d-math>\theta</d-math> that maximize
      <d-math>J^π_R</d-math> subject to the constraint <d-math>J^π_C \leq \alpha</d-math> but also minimize
      the Lagrangian multiplier <d-math>\lambda</d-math>:
    </p>
    <d-math>
      \min_{\lambda \geq 0}\max_{\theta} [J^{π_\theta}_R - \lambda (J^{π_\theta}_C - \alpha)]
    </d-math>

    <h2 id="RCPO">
      Reward Constrained Policy Optimization
    </h2>
    <p>
      Over the last years there has been a rise in the use of Actor-Critice based approaches such as PPO.
      The actor learns a policy π, whereas the critic learns the value using temporal difference learning.
      Originally, the use of the critic was done to reduce the variance but it also enables
      training using a finite number of samples.
    </p>

    <h3>
      How to integrate the constraint into the Actor-Critic approach?
    </h3>

    <p>
      One way to integrate the constraint into the Actor-Critic approach is using an alternative, guiding penalty
      - the discounted penalty.
    </p>

    <p>
      The first step to integrate the constraint into the Actor-Critic approach is to use the
      constraint objective <d-math>J^π_C</d-math> as the value function. This is simply done by reformulating it as
      the
      expected discounted sum of the constraint values:
    </p>

    <d-math>
      V^π_{C_\gamma}(s) \hat{=} \mathbb{E}^{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t c(s_t, a_t) | s_0 = s \right]
    </d-math>

    <p>
      Now we can simply include the constraint value to the reward function via the Lagrange multiplier:
    </p>

    <d-math>
      \hat{r} = r(s,a) - \lambda c(s,a)
    </d-math>

    <p>
      This is the guiding penalty. The constraint is now integrated into the reward function and the actor can learn
      to maximize the reward while considering the constraint.
    </p>

    <p>
      Updating the Lagrangian multiplier is done via the derivative of the global objective:
    </p>

    <d-math>
      <d-math>\lambda</d-math> <d-math>\gets</d-math> max(<d-math>\lambda</d-math> -
      lr_{<d-math>\lambda</d-math>}(-(\mathbb{E}^{\pi_\theta}_{s\sim\mu}
      \left[C\right] - \alpha)), 0)
    </d-math>
    <p>
      <d-math>lr_\lambda</d-math> is the learning rate for the Lagrangian multiplier. The max function is
      used to ensure
      that the Lagrangian multiplier is always positive.
    </p>

    <h2 id="Implementation">
      Implementation
    </h2>

    <p>
      We have decided to implement RCPO into the stable-baselines3<d-cite key="stable-baselines3"></d-cite> library.
      This is a library that provides a stable version of PPO which we extend to develop RCPPO which can be found in
      this <a href="https://github.com/sudo-Boris/stable-baselines3">sudo-Boris/stable-baselines3</a> github repo. We
      additionally use the gym environment and need to return the constraint value.
    </p>

    <h4>
      Integrating the guiding penalty
    </h4>

    <p>
      In PPO the computation of rewards (here called returns) the Temporal Difference Error (TD estimate) and
      the Generalized Advantage Estimation (GAE) advantage are necessary. <br>
      To integrate the constraint into the reward function, we need to add the Lagrangian-scaled constraint value to
      the reward as discussed in the <a href="./#RCPO">RCPO section</a>. This is done when computing the TD error
      estimate.
    </p>

    <d-figure id="code_snapshot">
      <figure>
        <img src="images/code-snapshot2_new.png" alt="Code snapshot of constraint integration into reward">
        <figcaption>
          The discussed integration of the constraint into the reward function is implemented into the computation of
          the advantages and returns. When the lambda parameter is set to 0, the constraint is ignored and the reward
          function is the same as in the original PPO implementation.
        </figcaption>
      </figure>
    </d-figure>


    <p>
      Evidently, it is also necessary to add a buffer for the constraint values. Moreover, providing those constraint
      values is done by customizing the gym environments to return the constraint values in the <d-math>info</d-math>
      dictionary.
    </p>

    <h4>
      Updating the Lagrangian multiplier
    </h4>

    <p>
      Due to the fact, that PPO (i) collects multiple episodes until the buffers are full and (ii) supports vectorized
      environments the logic for collecting and aggregating the constraint values across the episodes and parallel
      environments is a bit more complex. Nevertheless, we have chosen the aggregation method to be the average over
      all
      time steps in one complete episode and across all those episodes themselves, i.e. episodes that have reached a
      terminal state.
    </p>

    <d-figure id="code_snapshot_lambda_update">
      <figure>
        <img src="images/lambda_update2_new.png" alt="Code snapshot of Lagrangian update">
        <figcaption>
          After aggregating the constraint values across the episodes and parallel environments into self.C, the
          Lagrangian is updated. The max function is used to ensure that the Lagrangian multiplier is always positive.
        </figcaption>
      </figure>
    </d-figure>

    <h2 id="Experiments">
      Experiments
    </h2>

    <p>
      Tessler C. et al.<d-cite key="Tessler2018RCPO"></d-cite> have performed their experiments on a custom marse
      rover
      environment but also on the <a href="https://gymnasium.farama.org/environments/mujoco/">OpenAI MuJoCo Gym</a>
      environments. We have decided to focus on the HalfCheetah MuJoCo environment as it produces the most stable
      results. <br>
      In the Mujoco environments each robot is composed of n joints. At each step the agent selects the amount of
      torque
      that is applied to each if its joints and the goal is to move the robot forward as long as possible. To prolong
      the robots motor life we want to constrain it from using high torque values while still enabling the robot to
      perform the task at hand. <br>
      To accomplish this we define the constraint C as the average torque applied to each motor, and the per-state
      penalty c(s, a) is the torque applied at each time step.
    </p>

    <p>
      In the paper the researchers compare their RCPO algorithm to the reward shaping approach using PPO. In
      the latter case, a PPO simulation with a fixed penalty coefficient is used for each <d-math>\lambda</d-math>
      value. <br>
      We on the other hand implemented the RCPO approach into PPO directly and trained the policy and lambda on
      the HalfCheetah environment. The reward shaping approach in our case simply meant to set the
      <d-math>\lambda</d-math> value to a fixed value and train the policy on it from the start. <br>
    </p>

    <p>
      The results of the experiments are shown in the following figures. We kept (almost) all hyperparameters the
      same as well as in the original paper and let the agents train for <d-math>1,000,000</d-math> time-steps.
    </p>

    <d-figure id="experiments_results">
      <figure>
        <!-- <img src="images/experiments_results.png" alt="Experiment Results"> -->
        <img src="images/experiments_results_smooth_constraints.png" alt="Experiment Results">
        <figcaption>
          Rewards and average torque of the experiments on the HalfCheetah environment. The maximum torque constraint
          is represented by the dashed line.
        </figcaption>
      </figure>
    </d-figure>

    <p>
      The results show that the RCPPO approach is able to learn a policy that is able to walk forward while
      respecting the constraint. Furthermore, we not only achieve comparable results to the paper, but we even
      outperform the results from their RCPO model. <br>
      Interestingly, low <d-math>\lambda</d-math> values seem to be less stable than high <d-math>\lambda</d-math>
      values. The guiding penalty appears to not only be enforcing the constraint but also improves the learning
      process
      overall. This might also be due to the fact that the Neural network architecture used in the paper is small
      with its 2 layers of 64 hidden units instead of commonly used 256 hidden neurons. <br>
    </p>

    <h4>
      Deviating hyperparameters
    </h4>

    <p>
      Furthermore, we had to select higher values for the Lagrangian multiplier itself when performing
      reward shaping, thus also having to increase its respective learning rate when training it as a parameter, so
      that it can grow quicker. This leads to <d-math>lr_{\lambda}</d-math> being larger than
      <d-math>lr_{\pi}</d-math> which ignores one of the assumption made in the paper, yet leads to coherent, and
      better results!
      <br>
      E.g. in the paper a <d-math>\lambda</d-math> value of 0.1 is already very high as it leads to a reward of
      <d-math>-0.4</d-math> and torque of <d-math>0.1387</d-math>, whereas in our case a <d-math>\lambda</d-math>
      value of 1.0 leads to a reward of about <d-math>1 500</d-math> with an average torque of <d-math>0.39</d-math>.
      <br>
      <br>
      A reason for the slower and weaker impact of the constraint could be attributed to the clipping of the trust
      region. This is a technique to ensure that the policy does not change too much between updates and run the risk
      of
      landing in a bad local minimum it can not escape from. This is done by clipping the policy update to a certain
      range. Therefore, even with "high" values of lambda w.r.t. the original paper the policy will not change
      significantly to conform to the constraint.
    </p>

    <h4>
      Qualitative observations
    </h4>

    <p>
      Finally, we want to see if we can qualitatively observe the effect of the constraint on the policy. To do so, we
      have recorded videos of the agents walking forward with different <d-math>\lambda</d-math> values. The results
      can be seen below.
    </p>

    <d-figure id="experiments">
      <figure>
        <div id="experiments-target" class="video/mp4"></div>
        <figcaption>
          The lower the lambda, the more the constraint is ignored.
          The higher the lambda, the more the constraint is enforced and the main reward objective is ignored.
          At <d-math>\lambda \in \{10, 100\}</d-math> the robot applies <d-math>0</d-math> torque to completely oblige
          to the constraint ignoring the main reward objective to walk forward which is observable at <d-math>\lambda
            \in\{RCPPO, 0, 0.00001\}</d-math>. With <d-math>\lambda \in \{0, 0.00001\}</d-math> the robot is able to
          walk forward, but it is visible that it moves its legs much quicker and more aggressively than the RCPPO
          agent. Furthermore, the RCPPO agent walks perfectly, whilst the other (moving) agents tumble over their own
          hecktick steps.
        </figcaption>
      </figure>
    </d-figure>

    <!-- For whatever reason this needs to be included otherwise the viz above won't appear -->
    <d-figure id="svelte-example-dfigure">
    </d-figure>

    <h2 id="Discussion">
      Discussion
    </h2>

    <p>
      The results of the experiments show that the RCPO approach is able to learn a policy that is able to optimize
      the
      main reward objective while respecting the constraint. Furthermore, we not only achieve comparable results to
      the
      paper, but we even outperform the results from their RCPO model. <br>
      The results of the reward shaping approach are also comparable in terms of relative performance between the
      different <d-math>\lambda</d-math> values.<br>
    </p>

    <p>
      Safe Reinforcement Learning is a critical area of research in the field of artificial intelligence, as it has
      the potential to shape the future of autonomous systems in a multitude of domains, ranging from robotics to
      finance. <br>
      The complexer systems become the more difficult it is to ensure that they are safe, especially through
      simple reward shaping. An approach such as RCPO can help to ensure that the safety constraints are respected
      while
      enforcing them by only providing the constraint itself. <br>
    </p>

    <!-- <h3>
      Parameter updating frequency
    </h3>

    <p>
      To be able to reproduce the results in the paper we had to deviate from it in some aspects. <br><br>
      In our implementation, we only update the weights and the Lagrangian multiplier once after several episode
      instead of after every episode as in the paper. This is due to the implementation design of the buffers in
      stable-baselines3. As mentioned, those need to be filled before the update can be performed, i.e. there will
      most likely be multiple episodes in on buffer. On the other hand, we update the lambda parameter for n epochs,
      just like the policy and value function.
    </p>

    <h3>
      Deviating hyperparameters
    </h3>

    <p>
      Furthermore, we had to select higher values for the Lagrangian multiplier itself when performing
      reward shaping, thus also having to increase its respective learning rate when training it as a parameter, so
      that it can grow quicker. This leads to <d-math>lr_{\lambda}</d-math> being higher than
      <d-math>lr_{\pi}</d-math> which ignores one of the assumption made in the paper, yet leads to coherent, and
      better results!
      <br>
      E.g. in the paper a <d-math>\lambda</d-math> value of 0.1 is already very high as it leads to a reward of
      <d-math>-0.4</d-math> and torque of <d-math>0.1387</d-math>, whereas in our case a <d-math>\lambda</d-math>
      value
      of 1.0 leads to a reward of about <d-math>2,400</d-math> with an average torque of <d-math>0.34</d-math>.
      <br>
      <br>
      In addition to the frequency of updating the parameters, a reason for the slower and weaker impact of the
      constraint could be attributed to the clipping of the trust region. This is a technique to ensure that the
      policy
      does not change too much between updates and run the risk of landing in a bad local minimum it can not escape
      from. This is done by clipping the policy update to a certain range. Therefore, even with "high" values of
      lambda
      w.r.t. the original paper the policy will not change significantly to conform to the constraint.

    </p>

    <h3>
      The Drawbacks of Reward Shaping
    </h3>

    <p>
      Similar to the findings in the paper, we have also observed that the non-adaptive approach is prone to
      converge
      to
      sub-optimal solutions. This is observable when comparing the results of different attempts of training the
      policy
      with the same fixed <d-math> \lambda</d-math> value, e.g. 2. The results are shown in the following figure.
    </p> -->

    <d-appendix>
      <h3>Acknowledgments</h3>

      <p>
        We thank .... Funding (@Prof. Obermayer)
      </p>


      <d-footnote-list></d-footnote-list>
      <d-citation-list></d-citation-list>
    </d-appendix>

    <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
    <d-bibliography src="bibliography.bib"></d-bibliography>

</body>