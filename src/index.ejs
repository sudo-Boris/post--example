<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style id="distill-article-specific-styles">
    <%=require("../static/styles.css") %>
  </style>
  <script src="https://distill.pub/template.v2.js"></script>
</head>

<body>

  <d-front-matter>
    <script type="text/json">
      <%= JSON.stringify(require("./frontmatter.json"), null, 4) %>
    </script>
  </d-front-matter>

  <d-title>
    <h1>Adaptive Reward Shaping in Safe Reinforcement Learning</h1>
    <p>Reward Constrained Proximal Policy Optimization</p>
  </d-title>

  <d-article>

    <d-abstract class="l-page-outset">
      <p>
        In this blog, we dive into the ICLR 2019 paper "Reward Constrained Policy Optimization" by Tessler et al.
        and highlight the importance of adaptive reward shaping in safe reinforcement learning.

        We reproduce the paper's experimental results by implementing Reward Constrained Policy Optimization
        (RCPO)<d-cite key="Tessler2018RCPO"></d-cite> into Proximal Policy
        Optimization (PPO)<d-cite key="Schulman2017PPO"></d-cite>.

        The goal of this blog is to provide researchers and practitioners with (1) a better understanding of
        safe reinforcement learning in terms of
        <a href="http://www.mit.edu/~dimitrib/Constrained-Opt.pdf"><b>constrained optimization</b></a>
        , and (2) how reward shaping
        can be effectively used to train a robust policy.
      </p>
    </d-abstract>

    <div class="l-screen horizontal-line"></div>

    <d-contents id="toc">
      <div class="toc-wrapper">
        <nav class="figcaption" id="menu">
          <h4>Contents</h4>
          Introduction to Safe Reinforcement Learning
          <div><a href="./#Safe-RL">Introduction to Safe Reinforcement Learning</a></div>
          <div><a href="./#What-are-CMDPs">What are Constrained MDPs</a></div>
          <div><a href="./#CPO">Constrained Policy Optimization</a></div>
          <div><a href="./#RCPO">Reward Constrained Policy Optimization</a></div>
          <div><a href="./#Implementation">Implementation</a></div>
          <div><a href="./#Experiments">Experiments</a></div>
        </nav>
        <div class="toc-line"></div>
      </div>
    </d-contents>

    <h2 id="Safe-RL">
      Introduction to Safe Reinforcement Learning
    </h2>

      <p>
        Safe RL can be defined as the process of learning policies that maximize the expectation
        of the return in problems in which it is important to ensure reasonable system performance
        and/or respect safety constraints during the learning and/or the deployment processes <d-cite key="garcia_comprehensive_2015"></d-cite>.
      </p>
       <p>
         <b>Some other recent ICLR papers on Safe Reinforcement Learning:</b>
<br>
      <a href="https://openreview.net/pdf?id=HJgEMpVFwB">
              Adversarial Policies: Attacking Deep Reinforcement Learning</a>,
          Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, and Stuart Russell, <i>ICLR 2020</i>
<br>
      <a href="https://arxiv.org/pdf/2201.09802.pdf">
            Constrained Policy Optimization via Bayesian World Models</a>,
          Yarden As, Ilnura Usmanova, Sebastian Curi and Andreas Krause, <i>ICLR 2022</i>
<br>
      <a href="https://openreview.net/pdf?id=TBIzh9b5eaz">
            Risk-averse Offline Reinforcement Learning</a>,
          Núria Armengol Urpí, Sebastian Curi, and Andreas Krause, <i>ICLR 2021</i>
<br>
      <a href="https://openreview.net/pdf?id=S1vuO-bCW">
          Leave No Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning</a>,
          Benjamin Eysenbach, Shixiang Gu, Julian Ibarz, and Sergey Levine, <i>ICLR 2018</i>
<br>
      <a href="https://openreview.net/pdf?id=iaO86DUuKi">
          Conservative Safety Critics for Exploration</a>,
          Homanga Bharadhwaj, Aviral Kumar, Nicholas Rhinehart, Sergey Levine, Florian Shkurti, and Animesh Garg, <i>ICLR 2021</i>
<br>
      <a href="https://openreview.net/pdf?id=TQt98Ya7UMP">
          Balancing Constraints and Rewards with Meta-gradient D4PG</a>,
          Dan A. Calian, Daniel J. Mankowitz, Tom Zahavy, Zhongwen Xu, Junhyuk Oh, Nir Levine, and Timothy Mann, <i>ICLR 2021</i>
<br>
      </p>
    </h2>

    <h2 id="What-are-CMDPs">
      What are Constrained MDPs?
    </h2>
    <p>
      In Reinforcement Learning (RL) the world is modeled as a Markov Decision Process (MDP) and the goal is to
      optimize an objective <d-math>J^π</d-math>,
      which is defined as the expected discounted reward to come:
    </p>

    <d-math>
      J^π = \mathbb{E}_{s\sim\mu} \left[ \sum_{t=0}^T \gamma^t r(s_t, a_t) \right]
    </d-math>

    <p>
      where <d-math>\pi</d-math> is the policy, <d-math>\gamma</d-math> is the discount factor, and
      <d-math>r(s_t,a_t)</d-math> is the reward function.
    </p>

    <p>
      However, in many real-world applications, the objective is not only to maximize the reward, but also to satisfy
      some constraints. For example, in robotics, the objective is to maximize the reward (e.g. the distance traveled)
      while also satisfying the constraint (e.g. the robot should only apply a certain maximum amount
      of torque to its joints).
    </p>

    <p>
      Mathematically speaking, we introduce a new constraint objective and add new indices to differentiate
      between the reward objective <d-math>J^π_R</d-math> and the new constrain objective <d-math>J^π_C</d-math>.
      This objective generally is defined as the expected constraint value over N time steps
      <d-math>J^π_C = \mathbb{E}_{s\sim\mu} \left[ C(s) \right]</d-math>.
      The method of aggregating those individual constraint values over time can vary, e.g. using the average
      or the maximum constraint value over N time steps or again a discounted sum.
    </p>

    <p>
      Getting back to the example of the robot, the constraint objective could be defined as the average torque applied
      to the joints over N time steps. The maximum amount of torque applied, so the upper bound of the constraint, is
      given by the parameter <d-math>\alpha</d-math>.
    </p>

    <p>
      The final objective is now to maximize <d-math>J^π_R</d-math> subject to the constraint <d-math>J^π_C \leq
        \alpha</d-math>:
    </p>

    <d-math>
      \max_{\pi \in \Pi} J^π_R \text{ s.t. }
      J^π_C \leq \alpha
    </d-math>

    <h2 id="CPO">
      Constrained Policy Optimization
    </h2>
    <p>
      The problem of optimizing a constrained objective is known as Constrained Policy Optimization (CPO).
      The most common approach to CPO is to use a Lagrangian multiplier <d-math>\lambda</d-math> to
      enforce the constraint. With parameterized approaches such as Neural Networks the objective is then to
      find the networks parameters <d-math>\theta</d-math> that maximize <d-math>J^π_R</d-math> subject to the
      constraint
      <d-math>J^π_C \leq \alpha</d-math> given the Lagrangian multiplier <d-math>\lambda</d-math>:
    </p>
    <d-math>
      \max_{\theta} [J^{π_\theta}_R - \lambda (J^{π_\theta}_C - \alpha)]
    </d-math>

    <h3>
      What does the lambda <i><b>actually</b></i> do?
    </h3>
    <p>
      Intuitively, the Lagrangian multiplier <d-math>\lambda</d-math> is used to determine how much weight is put
      onto the constraint. If <d-math>\lambda</d-math> is set to 0, the constraint is ignored and the objective
      becomes the reward objective <d-math>J^π_R</d-math>. If <d-math>\lambda</d-math> is set very high, the
      constraint is enforced very strictly and the global objective function reduces to the constraint objective
      <d-math>J^π_C</d-math>.
    </p>

    <h4>
      Demonstrating the effect of the Lagrangian multiplier
    </h4>
    <p>
      Let's take a look at a simple example to demonstrate the effect of the Lagrangian multiplier.
      We'll use the simple CartPole Gym environment. The reward in this environment is +1 for every step the pole was
      kept upright.

      We can now add an example constraint to the environment. Let's say we want to keep the cart in the left quarter of
      the x-axis. We therefor define the constraint value to be the x-position of the cart and the upper bound
      <d-math>\alpha</d-math> to be -2.

    <p>
      Try out the different Lambda values in the slider below to see how the constraint is enforced.
    </p>

    <d-figure id="tuning-lambda">
      <figure>
        <div id="tuning-lambda-target" class="video/mp4"></div>
        <figcaption>
          The lower the lambda, the more the constraint is ignored.
          The higher the lambda, the more the constraint is enforced and the main reward objective is ignored.
          At <d-math>\lambda = 1,000,000</d-math> the cart shoots to the right to tilt the pole to the left but does so
          ignoring the following balancing act which is observable at <d-math>\lambda \in \{10, 100\}</d-math>.
        </figcaption>
      </figure>
    </d-figure>

    <h4>
      Setting the Lagrangian as a hyperparameter
    </h4>
    <p>
      One can tune the Lagrangian as a hyperparameter (which is then called reward shaping), but ideally one would want
      it to be learned itself to find the
      perfect balance between optimizing <d-math>J^π_R</d-math> and <d-math>J^π_C</d-math>!
      So the new and final objective is to find the networks parameters <d-math>\theta</d-math> that maximize
      <d-math>J^π_R</d-math> subject to the constraint <d-math>J^π_C \leq \alpha</d-math> but also minimize
      the Lagrangian multiplier <d-math>\lambda</d-math>:
    </p>
    <d-math>
      \min_{\lambda \geq 0}\max_{\theta} [J^{π_\theta}_R - \lambda (J^{π_\theta}_C - \alpha)]
    </d-math>

    <h2 id="RCPO">
      Reward Constrained Policy Optimization
    </h2>
    <p>
      Over the last years there has been a rise in the use of Actor-Critice based approaches such as PPO.
      The actor learns a policy π, whereas the critic learns the value using temporal difference learning.
      Originally, the use of the critic was done to reduce the variance but it also enables
      training using a finite number of samples.
    </p>

    <h3>
      How to integrate the constraint into the Actor-Critic approach?
    </h3>

    <p>
      One way to integrate the constraint into the Actor-Critic approach is using an alternative, guiding penalty
      - the discounted penalty.
    </p>

    <p>
      The first step to integrate the constraint into the Actor-Critic approach is to use the
      constraint objective <d-math>J^π_C</d-math> as the value function. This is simply done by reformulating it as the
      expected discounted sum of the constraint values:
    </p>

    <d-math>
      V^π_{C_\gamma}(s) \hat{=} \mathbb{E}^{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t c(s_t, a_t) | s_0 = s \right]
    </d-math>

    <p>
      Now we can simply include the constraint value to the reward function via the Lagrange multiplier:
    </p>

    <d-math>
      \hat{r} = r(s,a) - \lambda c(s,a)
    </d-math>

    <p>
      This is the guiding penalty. The constraint is now integrated into the reward function and the actor can learn
      to maximize the reward while considering the constraint.
    </p>

    <p>
      Updating the Lagrangian multiplier is done via the derivative of the global objective:
    </p>

    <d-math>
      <d-math>\lambda</d-math> <d-math>\gets</d-math> max(<d-math>\lambda</d-math> -
      lr_{<d-math>\lambda</d-math>}(\mathbb{E}^{\pi_\theta}_{s\sim\mu}
      \left[C\right] - \alpha), 0)
    </d-math>
    <p>
      <d-math>lr_\lambda</d-math> is the learning rate for the Lagrangian multiplier. The max function is
      used to ensure
      that the Lagrangian multiplier is always positive.
    </p>

    <h2 id="Implementation">
      Implementation
    </h2>

    <p>
      We have decided to implement RCPO into the stable-baselines3<d-cite key="stable-baselines3"></d-cite> library.
      This is a library that provides a stable version of PPO which we extend to develop RCPPO which can be found in
      this <a href="https://github.com/sudo-Boris/stable-baselines3">sudo-Boris/stable-baselines3</a> github repo. We
      additionally use the gym environment and need to return the constraint value.
    </p>

    <p>
      The discussed integration of the constraint into the reward function is implemented into the computation of the
      advantages and returns.
    </p>
    <pre>delta = self.rewards[step] - self.constraint_lambda * self.constraints[step]
      + self.gamma * next_values * next_non_terminal - self.values[step]
    </pre>

    <p>
      Evidently, it is also necessary to add a buffer for the constraint values. Moreover, providing those constraint
      values is done by customizing the gym environments to return the constraint values in the <d-math>info</d-math>
      dictionary.
    </p>

    <p>
      Due to the fact, that PPO (i) collects multiple episodes until the buffers are full and (ii) supports vectorized
      environments the logic for collecting and aggregating the constraint values across the episodes and parallel
      environments is a bit more complex. Nevertheless, we have chosen the aggregation method to be the average over all
      time steps in an episode and across all episodes themselves.
    </p>

    <h2 id="Experiments">
      Experiments
    </h2>

    <p>
      Tessler C. et al.<d-cite key="Tessler2018RCPO"></d-cite> have performed their experiments on a custom marse rover
      environment but also on the <a href="https://gymnasium.farama.org/environments/mujoco/">OpenAI MuJoCo Gym</a>
      environments. We have decided to focus on the HalfCheetah MuJoCo environment as it produces the most stable
      results. <br>
      In the Mujoco environments each robot is composed of n joints. At each step the agent selects the amount of torque
      that is applied to each if its joints and the goal is to move the robot forward as long as possible. To prolong
      the robots motor life we want to constrain it from using high torque values while still enabling the robot to
      perform the task at hand. <br>
      To accomplish this we define the constraint C as the average torque applied to each motor, and the per-state
      penalty c(s, a) is the torque applied at each time step.
    </p>

    <p>
      In the paper the researchers compare their RCPO algorithm to the reward shaping approach using PPO. In
      the latter case, a PPO simulation with a fixed penalty coefficient is used for each <d-math>\lambda</d-math>
      value. <br>
      We on the other hand implemented the RCPO approach into (RC)PPO directly and trained the policy and lambda on the
      HalfCheetah environment. The reward shaping approach in our case simply meant to set the <d-math>\lambda</d-math>
      value to a fixed value and train the policy on it from the start. <br>
    </p>

    <p>
      The results of the experiments are shown in the following figures. We kept (almost) all hyperparameters the
      same as well as in the original paper and let the agents train for <d-math>1,000,000</d-math> time-steps.
    </p>

    <!-- Insert two images side by side -->

    <d-figure id="experiments_reward">
      <figure>
        <img src="images/experiments_reward.png" alt="Rewards">
        <figcaption>
          Rewards of the experiments on the HalfCheetah environment.
        </figcaption>
      </figure>
    </d-figure>

    <d-figure id="experiments_constraints">
      <figure>
        <img src="images/experiments_constraints.png" alt="Constraints">
        <figcaption>
          Average constraint results of the experiments on the HalfCheetah environment. The dashed line represents the
          maximal allowed constraint value.
        </figcaption>
      </figure>
    </d-figure>

    <p>
      The results show that the RCPPO approach is able to learn a policy that is able to walk forward while
      respecting the constraint. Furthermore, we not only achieve comparable results to the paper, but we even
      outperform the results from their RCPO model. <br>
      The results of the reward shaping approach are also comparable in terms of relative performance between the
      different <d-math>\lambda</d-math> values. <d-math>\lambda = 0.00001</d-math> has a higher reward but also higher
      average torque than <d-math>\lambda = 0</d-math>. <br>
      For reference, the results of the RCPO approach are shown in the following figure.
    </p>

    <d-figure id="experiments_rcppo">
      <figure>
        <img src="images/Paper_Results.png" alt="RCPO_Results" style="display: inline-block;">
        <figcaption>
          Results from the original paper.
        </figcaption>
      </figure>
    </d-figure>

    <p>
      Finally, we want to see if we can qualitatively observe the effect of the constraint on the policy. To do so, we
      have recorded videos of the agents walking forward with different <d-math>\lambda</d-math> values. The videos can
      be seen below.
    </p>

    <d-figure id="experiments">
      <figure>
        <div id="experiments-target" class="video/mp4"></div>
        <figcaption>
          The lower the lambda, the more the constraint is ignored.
          The higher the lambda, the more the constraint is enforced and the main reward objective is ignored.
          At <d-math>\lambda = 100</d-math> the robot applies <d-math>0</d-math> torque to completely oblige to the
          constraint ignoring the main objective to walk forward which is observable at <d-math>\lambda \in
            \{"learned",
            0, 1e-5, 1\}</d-math>.
        </figcaption>
      </figure>
    </d-figure>

    <!-- For whatever reason this needs to be included otherwise the viz above won't appear -->
    <d-figure id="svelte-example-dfigure">
    </d-figure>

    <h2 id="Discussion">
      Discussion
    </h2>

    <h3>
      Parameter updating frequency
    </h3>

    <p>
      To be able to reproduce the results in the paper we had to deviate from it in some aspects. <br><br>
      In our implementation, we only update the weights and the Lagrangian multiplier once after several episode
      instead of after every episode as in the paper. This is due to the implementation design of the buffers in
      stable-baselines3. As mentioned, those need to be filled before the update can be performed, i.e. there will
      most likely be multiple episodes in on buffer. On the other hand, we update the lambda parameter for n epochs,
      just like the policy and value function.
    </p>

    <h3>
      Deviating hyperparameters
    </h3>

    <p>
      Furthermore, we had to select higher values for the Lagrangian multiplier itself when performing
      reward shaping, thus also having to increase its respective learning rate when training it as a parameter, so
      that it can grow quicker. This leads to <d-math>lr_{\lambda}</d-math> being higher than
      <d-math>lr_{\pi}</d-math> which ignores one of the assumption made in the paper, yet leads to coherent, and
      better results!
      <br>
      E.g. in the paper a <d-math>\lambda</d-math> value of 0.1 is already very high as it leads to a reward of
      <d-math>-0.4</d-math> and torque of <d-math>0.1387</d-math>, whereas in our case a <d-math>\lambda</d-math>
      value
      of 1.0 leads to a reward of about <d-math>2,400</d-math> with an average torque of <d-math>0.34</d-math>.
      <br>
      <br>
      In addition to the frequency of updating the parameters, a reason for the slower and weaker impact of the
      constraint could be attributed to the clipping of the trust region. This is a technique to ensure that the
      policy
      does not change too much between updates and run the risk of landing in a bad local minimum it can not escape
      from. This is done by clipping the policy update to a certain range. Therefore, even with "high" values of
      lambda
      w.r.t. the original paper the policy will not change significantly to conform to the constraint.

    </p>

    <h3>
      The Drawbacks of Reward Shaping
    </h3>

    <p>
      Similar to the findings in the paper, we have also observed that the non-adaptive approach is prone to
      converge
      to
      sub-optimal solutions. This is observable when comparing the results of different attempts of training the
      policy
      with the same fixed <d-math> \lambda</d-math> value, e.g. 2. The results are shown in the following figure.
    </p>

    <d-appendix>
      <h3>Acknowledgments</h3>

      <p>
        We thank .... Funding (@Prof. Obermayer)
      </p>


      <d-footnote-list></d-footnote-list>
      <d-citation-list></d-citation-list>
    </d-appendix>

    <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
    <d-bibliography src="bibliography.bib"></d-bibliography>

</body>