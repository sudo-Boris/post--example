<!doctype html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style id="distill-article-specific-styles">
    <%=require("../static/styles.css") %>
  </style>
  <script src="https://distill.pub/template.v2.js"></script>
</head>

<body>

  <d-front-matter>
    <script type="text/json">
      <%= JSON.stringify(require("./frontmatter.json"), null, 4) %>
    </script>
  </d-front-matter>

  <d-title>
    <h1>Reward Constrained Proximal Policy Optimization</h1>
    <p>Learning the perfect balance between constraint and reward.</p>
  </d-title>

  <d-article>

    <d-abstract class="l-page-outset">
      <p>
        In this article, we demonstrate the implementation of Reward Constrained Policy Optimization
        (RCPO)<d-cite key="Tessler2018RCPO"></d-cite> into stable-baselines implementation of Proximal Policy
        Optimization (PPO)<d-cite key="Schulman2017PPO"></d-cite>.
        We show that RCPO can be used to learn the perfect balance between
        constraint and reward, and that it can be used to learn a policy that is robust to the constraint.
      </p>
    </d-abstract>

    <div class="l-screen horizontal-line"></div>

    <d-contents id="toc">
      <div class="toc-wrapper">
        <nav class="figcaption" id="menu">
          <h4>Contents</h4>
          <div><a href="./#What-are-CMDPs">What are Constrained MDPs</a></div>
          <div><a href="./#CPO">Constrained Policy Optimization</a></div>
          <div><a href="./#RCPO">Reward Constrained Policy Optimization</a></div>
          <div><a href="./#Implementation">Implementation</a></div>
          <div><a href="./#Experiments">Experiments</a></div>
        </nav>
        <div class="toc-line"></div>
      </div>
    </d-contents>

    <h2 id="What-are-CMDPs">
      What are Constrained MDPs?
    </h2>
    <p>
      In Reinforcement Learning (RL) the world is modeled as a Markov Decision Process (MDP) and the goal is to
      optimize an objective <d-math>J^π</d-math>,
      which is defined as the expected discounted reward to come:
    </p>

    <d-math>
      J^π = \mathbb{E}_{s\sim\mu} \left[ \sum_{t=0}^T \gamma^t r(s_t, a_t) \right]
    </d-math>

    <p>
      where <d-math>\pi</d-math> is the policy, <d-math>\gamma</d-math> is the discount factor, and
      <d-math>r(s_t,a_t)</d-math> is the reward function.
    </p>

    <p>
      However, in many real-world applications, the objective is not only to maximize the reward, but also to satisfy
      some constraints. For example, in robotics, the objective is to maximize the reward (e.g. the distance traveled)
      while also satisfying the constraint (e.g. the robot should only apply a certain maximum amount
      of torque to its joints).
    </p>

    <p>
      Mathematically speaking, we introduce a new constraint objective and add new indices to differentiate
      between the reward objective <d-math>J^π_R</d-math> and the new constrain objective <d-math>J^π_C</d-math>.
      This objective generally is defined as the expected constraint value over N time steps
      <d-math>J^π_C = \mathbb{E}_{s\sim\mu} \left[ C(s) \right]</d-math>.
      The method of aggregating those individual constraint values over time can vary, e.g. using the average
      or the maximum constraint value over N time steps or again a discounted sum.
    </p>

    <p>
      Getting back to the example of the robot, the constraint objective could be defined as the average torque applied
      to the joints over N time steps. The maximum amount of torque applied, so the upper bound of the constraint, is
      given by the parameter <d-math>\alpha</d-math>.
    </p>

    <p>
      The final objective is now to maximize <d-math>J^π_R</d-math> subject to the constraint <d-math>J^π_C \leq
        \alpha</d-math>:
    </p>

    <d-math>
      \max_{\pi \in \Pi} J^π_R \text{ s.t. }
      J^π_C \leq \alpha
    </d-math>

    <h2 id="CPO">
      Constrained Policy Optimization
    </h2>
    <p>
      The problem of optimizing a constrained objective is known as Constrained Policy Optimization (CPO).
      The most common approach to CPO is to use a Lagrangian multiplier <d-math>\lambda</d-math> to
      enforce the constraint. With parameterized approaches such as Neural Networks the objective is then to
      find the networks parameters <d-math>\theta</d-math> that maximize <d-math>J^π_R</d-math> subject to the
      constraint
      <d-math>J^π_C \leq \alpha</d-math> given the Lagrangian multiplier <d-math>\lambda</d-math>:
    </p>
    <d-math>
      \max_{\theta} [J^{π_\theta}_R - \lambda (J^{π_\theta}_C - \alpha)]
    </d-math>

    <h3>
      What does the lambda <i><b>actually</b></i> do?
    </h3>
    <p>
      Intuitively, the Lagrangian multiplier <d-math>\lambda</d-math> is used to determine how much weight is put
      onto the constraint. If <d-math>\lambda</d-math> is set to 0, the constraint is ignored and the objective
      becomes the reward objective <d-math>J^π_R</d-math>. If <d-math>\lambda</d-math> is set very high, the
      constraint is enforced very strictly and the global objective function reduces to the constraint objective
      <d-math>J^π_C</d-math>.
    </p>

    <h4>
      Demonstrating the effect of the Lagrangian multiplier
    </h4>
    <p>
      Let's take a look at a simple example to demonstrate the effect of the Lagrangian multiplier.
      We'll use the simple CartPole Gym environment. The reward in this environment is +1 for every step the pole was
      kept upright.

      We can now add an example constraint to the environment. Let's say we want to keep the cart in the left quarter of
      the x-axis. We therefor define the constraint value to be the x-position of the cart and the upper bound
      <d-math>\alpha</d-math> to be -2.

    <p>
      Try out the different Lambda values in the slider below to see how the constraint is enforced.
    </p>

    <d-figure id="tuning-lambda">
      <figure>
        <div id="tuning-lambda-target" class="video/mp4"></div>
        <figcaption>
          The lower the lambda, the more the constraint is ignored.
          The higher the lambda, the more the constraint is enforced and the main reward objective is ignored.
          At <d-math>\lambda = 1,000,000</d-math> the cart shoots to the right to tilt the pole to the left but does so
          ignoring the following balancing act which is observable at <d-math>\lambda \in \{10, 100\}</d-math>.
        </figcaption>
      </figure>
    </d-figure>

    <!-- For whatever reason this needs to be included otherwise the viz above won't appear -->
    <d-figure id="svelte-example-dfigure">
    </d-figure>

    <h4>
      Setting the Lagrangian as a hyperparameter
    </h4>
    <p>
      One can tune the Lagrangian as a hyperparameter (which is then called reward shaping), but ideally one would want
      it to be learned itself to find the
      perfect balance between optimizing <d-math>J^π_R</d-math> and <d-math>J^π_C</d-math>!
      So the new and final objective is to find the networks parameters <d-math>\theta</d-math> that maximize
      <d-math>J^π_R</d-math> subject to the constraint <d-math>J^π_C \leq \alpha</d-math> but also minimize
      the Lagrangian multiplier <d-math>\lambda</d-math>:
    </p>
    <d-math>
      \min_{\lambda \geq 0}\max_{\theta} [J^{π_\theta}_R - \lambda (J^{π_\theta}_C - \alpha)]
    </d-math>

    <h2 id="RCPO">
      Reward Constrained Policy Optimization
    </h2>
    <p>
      Over the last years there has been a rise in the use of Actor-Critice based approaches such as PPO.
      The actor learns a policy π, whereas the critic learns the value using temporal difference learning.
      Originally, the use of the critic was done to reduce the variance but it also enables
      training using a finite number of samples.
    </p>

    <h3>
      How to integrate the constraint into the Actor-Critic approach?
    </h3>

    <p>
      One way to integrate the constraint into the Actor-Critic approach is using an alternative, guiding penalty
      - the discounted penalty.
    </p>

    <p>
      The first step to integrate the constraint into the Actor-Critic approach is to use the
      constraint objective <d-math>J^π_C</d-math> as the value function. This is simply done by reformulating it as the
      expected discounted sum of the constraint values:
    </p>

    <d-math>
      V^π_{C_\gamma}(s) \hat{=} \mathbb{E}^{\pi} \left[ \sum_{t=0}^{\infty} \gamma^t c(s_t, a_t) | s_0 = s \right]
    </d-math>

    <p>
      Now we can simply include the constraint value to the reward function via the Lagrange multiplier:
    </p>

    <d-math>
      \hat{r} = r(s,a) - \lambda c(s,a)
    </d-math>

    <p>
      This is the guiding penalty. The constraint is now integrated into the reward function and the actor can learn
      to maximize the reward while considering the constraint.
    </p>

    <p>
      Updating the Lagrangian multiplier is done via the derivative of the global objective:
    </p>

    <d-math>
      <d-math>\lambda</d-math> <d-math>\gets</d-math> max(<d-math>\lambda</d-math> -
      lr_{<d-math>\lambda</d-math>}(\mathbb{E}^{\pi_\theta}_{s\sim\mu}
      \left[C\right] - \alpha), 0)
    </d-math>
    <p>
      <d-math>lr_\lambda</d-math> is the learning rate for the Lagrangian multiplier. The max function is
      used to ensure
      that the Lagrangian multiplier is always positive.
    </p>

    <h2 id="Implementation">
      Implementation
    </h2>

    <p>
      We have decided to implement RCPO into the stable-baselines3<d-cite key="stable-baselines3"></d-cite> library.
      This is a library that provides a stable version of PPO which we extend to develop RCPPO which can be found in
      this <a href="https://github.com/sudo-Boris/stable-baselines3">sudo-Boris/stable-baselines3</a> github repo. We
      additionally use the gym environment and need to return the constraint value.
    </p>

    <p>
      The discussed integration of the constraint into the reward function is implemented in when computing the
      advantages and returns.
    </p>
    <pre>delta = self.rewards[step] - self.constraint_lambda * self.constraints[step]
      + self.gamma * next_values * next_non_terminal - self.values[step]
    </pre>

    <p>
      Evidently, it is also necessary to add a buffer for the constraint values. Moreover, providing those constraint
      values is done by customizing the gym environments to return the constraint values in the <d-math>info</d-math>
      dictionary.
    </p>

    <p>
      Due to the fact, that PPO (i) collects multiple episodes until the buffers are full and (ii) supports vectorized
      environments the logic for collecting and aggregating the constraint values across the episodes and parallel
      environments is a bit more complex. Nevertheless, we have chosen the aggregation method to be the average over all
      time steps in an episode and across all episodes themselves.
    </p>

    <h2 id="Experiments">
      Experiments
    </h2>

    <h2 id="Discussion">
      Discussion
    </h2>

    <p>
      To be able to reproduce the results in the paper we had to deviate from it in some aspects.
    </p>

    <h3>
      Parameter updating frequency
    </h3>

    <p>
      In our implementation, we only update the weights and the Lagrangian multiplier once after several episode
      instead of after every episode as in the paper. This is due to the implementation design of the buffers in
      stable-baselines3. As mentioned, those need to be filled before the update can be performed, i.e. there will most
      likely be multiple episodes in on buffer.
    </p>

    <h3>
      Deviating hyperparameters
    </h3>

    <p>
      Furthermore, we had to select higher values for the Lagrangian multiplier itself when performing
      reward shaping and its respective learning rate when training it as a parameter, so that it can grow quicker. <br>
      Nevertheless, because the Lagrangian needs to be so much larger, we also had to train 3x longer (3,000,000 steps
      instead of 1,000,000 steps) to achieve comparable results.
      <br>
      <br>
      In addition to the frequency of updating the parameters, a reason for the slower impact of the constraint could be
      attributed to the clipping of the trust region. This is a technique to ensure that the policy does not change too
      much between updates and run the risk of landing in a bad local minimum it can not escape from. This is done by
      clipping the policy update to a certain range. Therefore, even with "high" values of lambda w.r.t. the original
      paper the policy will not change significantly to conform to the constraint.

    </p>

    <p>
      - Can reproduce results, but need higher lambda
      - Might be because of Trust region clipping etc. -> Thus, slower/ weaker updating of policy and enforcing of
      constraint
      - To combat this, we set the learning rate for the Lagrangian a lot higher
    </p>

    <d-appendix>
      <h3>Acknowledgments</h3>
      <p>
        We are deeply grateful to Rong Guo for providing us with the opportunity to work on this project and for her
        guidance and support.
      </p>

      <h3>Author Contributions</h3>
      <p>
        <b>Research:</b> Boris and Tuan developed the code. Boris did the experiments to reproduce the results.
      </p>

      <p>
        <b>Writing & Visualizations:</b> Boris created the visualizations and wrote the blog post.
      </p>


      <d-footnote-list></d-footnote-list>
      <d-citation-list></d-citation-list>
    </d-appendix>

    <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
    <d-bibliography src="bibliography.bib"></d-bibliography>

</body>